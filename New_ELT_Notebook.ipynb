{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Jupyter Notebook Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71cfc2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6baa106e5f54e509697346161bf8451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1628537407081_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-111.ec2.internal:20888/proxy/application_1628537407081_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-1AC2YHCMIJNJN\n",
       "\" application-id=\"application_1628537407081_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-3-167.ec2.internal:8042/node/containerlogs/container_1628537407081_0001_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/01/3b/5fa91376d373cc6c91a2554f8bf4334cb275250539472f14e5dc971df0c4/boto3-1.18.17-py3-none-any.whl (131kB)\n",
      "Collecting botocore<1.22.0,>=1.21.17 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/f4/c98c4b194ac9552de7741372bcbdcd290d93fda377e527546e9868865948/botocore-1.21.17.tar.gz (8.0MB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/84/fc3717a7b7f0f6bb08af593127171f08e3e0087c197922da09c01bfe7c3a/s3transfer-0.5.0-py3-none-any.whl (79kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.22.0,>=1.21.17->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl (247kB)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.22.0,>=1.21.17->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/64/43575537846896abac0b15c3e5ac678d787a4021e906703f1766bfb8ea11/urllib3-1.26.6-py2.py3-none-any.whl (138kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.17->boto3)\n",
      "Building wheels for collected packages: botocore\n",
      "  Running setup.py bdist_wheel for botocore: started\n",
      "  Running setup.py bdist_wheel for botocore: finished with status 'done'\n",
      "  Stored in directory: /var/lib/livy/.cache/pip/wheels/97/6d/3b/30db091d38bcf5cddfc17d38c7c0b547fe2d3af8d05aeb9fa1\n",
      "Successfully built botocore\n",
      "Installing collected packages: python-dateutil, urllib3, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.18.17 botocore-1.21.17 python-dateutil-2.8.2 s3transfer-0.5.0 urllib3-1.26.6\n",
      "\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/92/96/144f70b972a9c0eabbd4391ef93ccd49d0f2747f4f6a2a2738e99e5adc65/requests-2.26.0-py2.py3-none-any.whl (62kB)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\" (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/77/ff688d1504cdc4db2a938e2b7b9adee5dd52e34efbd2431051efc9984de9/idna-3.2-py3-none-any.whl (59kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/tmp/1628538313131-0/lib/python3.7/site-packages (from requests)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\" (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/53/b7f6126a2b9fd878b025fe3c40266cfaad696f312165008ce045bffa3fe7/charset_normalizer-2.0.4-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/1b/0a0dece0e8aa492a6ec9e4ad2fe366b511558cdc73fd3abc82ba7348e875/certifi-2021.5.30-py2.py3-none-any.whl (145kB)\n",
      "Installing collected packages: idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2021.5.30 charset-normalizer-2.0.4 idna-3.2 requests-2.26.0\n",
      "\n",
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/site-packages (from bs4)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Building wheels for collected packages: bs4\n",
      "  Running setup.py bdist_wheel for bs4: started\n",
      "  Running setup.py bdist_wheel for bs4: finished with status 'done'\n",
      "  Stored in directory: /var/lib/livy/.cache/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4, soupsieve\n",
      "Successfully installed bs4-0.0.1 soupsieve-2.2.1"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"requests\")\n",
    "sc.install_pypi_package(\"bs4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ed7b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff053febab24b61a2d4e23c7948de68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "from boto3 import client\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6ff24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bd03bd04924de88d6f30347d13b30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start Spark context manager\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"OE Capstone App\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32ba1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b21b3276d54e2e826aa896d554e151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Pipeline Class\n",
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    This DataPipeline class will simply be used to take an website url as a data source and\n",
    "    an S3 object as a possible data storage destination. An EL method is also created with\n",
    "    description as follows, in addition to a single method to read and transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, s3_cli_object, s3_resource, s3_bucket, bucket_path, url):\n",
    "        \"\"\"\n",
    "        For this method, we can either have some s3_obj or a cloud path variable.\n",
    "        We can set these values to None first, then change it later.\n",
    "        \"\"\"\n",
    "        self.s3_cli_object = s3_cli_object\n",
    "        self.s3_resource = s3_resource\n",
    "        self.s3_bucket_name = s3_bucket\n",
    "        self.bucket_path = bucket_path\n",
    "        self.url = url\n",
    "\n",
    "    def extract_load(self):\n",
    "        \"\"\"\n",
    "        Function has three main steps to perform ELT based on \"use_cloud\" boolean parameter:\n",
    "        1) Extract (scrape) data from GDELT website HTML document.\n",
    "        2. Load data into S3 Bucket\n",
    "        \"\"\"\n",
    "        # Log to file\n",
    "        logging.basicConfig(filename='elt.log', level=logging.INFO)\n",
    "        # Extract data from HTML webpage\n",
    "        url = self.url\n",
    "        page = requests.get(url)\n",
    "        # Create BeautifulSoup object to parse through HTML document\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        # Base url for file downloading\n",
    "        base_url = url[:-10]\n",
    "        # Current Directory -> we do not need for Step 6.\n",
    "        current_directory = os.getcwd()\n",
    "        # Cloud Path\n",
    "        cloud_path = self.bucket_path\n",
    "        # Create S3 client instance\n",
    "        s3_client = self.s3_cli_object\n",
    "        # Bucket name\n",
    "        destination_bucket = self.s3_bucket_name\n",
    "\n",
    "        # Iterate to obtain all we want to download ALL DATA FROM HTML Document\n",
    "        # based on \"a\" and \"href\" tags in order to extract data, use PySpark to efficiently process data\n",
    "        # and save the data to new CSV file inside AWS S3 bucket.\n",
    "        \n",
    "        # UNCOMMENT FOR TO SCRAPE ALL\n",
    "        # for link in soup.find_all(\"a\")[2:]:\n",
    "        # UNCOMMENT TO SCRAPE FIRST ~1000 files.\n",
    "#        for link in soup.find_all(\"a\")[2:1000]:\n",
    "        # Attempt to scrape the next 1000\n",
    "        for link in soup.find_all(\"a\")[1000:2000]:\n",
    "            try:\n",
    "                if link.has_attr(\"href\"):\n",
    "                    # Obtain file name as string object\n",
    "                    file = link.attrs[\"href\"]\n",
    "                    download_url = f\"{base_url}{file}\"\n",
    "                    csv_file_str = file[:-4]\n",
    "                    print(csv_file_str)\n",
    "                    # Request data from URL and extract data as CSV to current (S3) directory using EMR Notebook\n",
    "                    response = requests.get(download_url)\n",
    "                    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "                    # zip_file.extractall()\n",
    "\n",
    "                    # Similar to MSharp's approach --> Play w/ response.content/parsed HTML document instead.\n",
    "                    data = zip_file.read(csv_file_str)\n",
    "                    local_path = \"/tmp/\" + csv_file_str\n",
    "                    s3_key = csv_file_str\n",
    "                    # Write to local path --> Think how to do this inside EMR Cluster. I will create tmp directory.\n",
    "                    with open(local_path, \"wb\") as f:\n",
    "                        f.write(data)\n",
    "                        del data\n",
    "                        print(\"Created csv file and removed original zip file object.\")\n",
    "                    # S3 Bucket\n",
    "                    s3_client.upload_file(local_path, destination_bucket, s3_key)\n",
    "                    print(\"File uploaded to S3 bucket.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error was {e}\")\n",
    "                \n",
    "    def spark_read_transform(self):\n",
    "        \"\"\"\n",
    "        1. Search for proper S3 bucket.\n",
    "        2. Read both files using Spark.\n",
    "        3. Apply data transformations.\n",
    "        4. Save to CSV file.\n",
    "        \"\"\"        \n",
    "        # Create s3 resource object\n",
    "        s3 = self.s3_resource\n",
    "\n",
    "        # Iterate through each file in desired S3 bucket\n",
    "        for bucket in s3.buckets.all():\n",
    "            # print(bucket)\n",
    "            if bucket == s3.Bucket(name='oecbucket2'):\n",
    "                print(\"OEC.\")\n",
    "                # Try to do more stuff.\n",
    "                for key in bucket.objects.all():\n",
    "                    # print(key.key)\n",
    "                    if \".gkgcounts.csv\" in key.key:\n",
    "                        # print(key.key)\n",
    "                        # Test to see if I can at least read a file called key.key:\n",
    "                        file_path = self.bucket_path + key.key\n",
    "                        spark_df2 = spark.read.csv(file_path, header=True, sep=\"\\t\")\n",
    "                        print(\"Showing results of Spark DataFrame 2: \\n\")\n",
    "\n",
    "#                         # FILTER OUT BAD FILES\n",
    "#                         spark_df2_columns = [\"DATE\", \"NUMARTS\", \"COUNTTYPE\", \"NUMBER\", \"OBJECTTYPE\", \"GEO_TYPE\", \"GEO_FULLNAME\",\n",
    "#                                             \"GEO_COUNTRYCODE\", \"GEO_ADM1CODE\", \"GEO_LAT\", \"GEO_LONG\", \"GEO_FEATUREID\", \"CAMEOEVENTIDS\",\n",
    "#                                             \"SOURCES\", \"SOURCEURLS\"]\n",
    "#                         # IF VALUES EXIST INSIDE SPARK DATAFRAME, OR IF SPARK DATAFRAME COLUMNS EXIST INSIDE COLUMNS LIST?\n",
    "#                         counter = 0\n",
    "#                         for col in spark_df2.columns:\n",
    "#                             counter += 1\n",
    "#                             if col not in spark_df2_columns:\n",
    "#                                 # Save file to desired location\n",
    "#                                 save_location = \"s3://oecbucket2/Bad_Files/\"\n",
    "#                                 file_location = save_location + key.key\n",
    "#                                 spark_df2.repartition(1).write.csv(path=file_location, header=True, sep=\"\\t\")\n",
    "#                                 print(\"Wrote to CSV File.\")\n",
    "#                                 counter = 0  # Reset counter to zero so that I can use logic to filter for next step.\n",
    "#                                 break\n",
    "                            \n",
    "#                         # Transformations (if counter has gone through all columns for filtering)\n",
    "#                         if counter == (len(spark_df2_columns) - 1):\n",
    "#                             spark_df2 = spark_df2.withColumn(\"NUMARTS\", spark_df2[\"NUMARTS\"].cast(\"int\"))\\\n",
    "#                                     .withColumn(\"NUMBER\", spark_df2[\"NUMBER\"].cast(\"int\"))\\\n",
    "#                                     .withColumn(\"GEO_TYPE\", spark_df2[\"GEO_TYPE\"].cast(\"int\"))\\\n",
    "#                                     .withColumn(\"GEO_LAT\", spark_df2[\"GEO_LAT\"].cast(\"float\"))\\\n",
    "#                                     .withColumn(\"GEO_LONG\", spark_df2[\"GEO_LONG\"].cast(\"float\"))\\\n",
    "#                                     .drop(\"DATE\")\n",
    "#                             save_location = \"s3://openendedcapstone/Data_Stream_2/\"\n",
    "#                             file_location = save_location + key.key\n",
    "#                             spark_df2.repartition(1).write.csv(path=file_location, header=True, sep=\"\\t\")\n",
    "#                             print(\"Wrote to CSV file.\")\n",
    "                            \n",
    "                        # COMMENT OUT (FOR NOT FILTERING OUT BAD FILES).\n",
    "#\n",
    "#                        \n",
    "                        # Transformations\n",
    "                        spark_df2 = spark_df2.withColumn(\"NUMARTS\", spark_df2[\"NUMARTS\"].cast(\"int\"))\\\n",
    "                                .withColumn(\"NUMBER\", spark_df2[\"NUMBER\"].cast(\"int\"))\\\n",
    "                                .withColumn(\"GEO_TYPE\", spark_df2[\"GEO_TYPE\"].cast(\"int\"))\\\n",
    "                                .withColumn(\"GEO_LAT\", spark_df2[\"GEO_LAT\"].cast(\"float\"))\\\n",
    "                                .withColumn(\"GEO_LONG\", spark_df2[\"GEO_LONG\"].cast(\"float\"))\\\n",
    "                                .drop(\"DATE\")\n",
    "#                         spark_df2.limit(3).show()\n",
    "                        save_location = \"s3://oecbucket2/Data_Stream_2/\"\n",
    "                        file_location = save_location + key.key\n",
    "                        spark_df2.repartition(1).write.csv(path=file_location, header=True, sep=\"\\t\")\n",
    "                        print(\"Wrote to CSV file.\")\n",
    "                    elif \".gkg.csv\" in key.key:\n",
    "                        # Apply data transformations:\n",
    "                        # 1) Drop \"COUNTS\" column.\n",
    "                        # 2) Change NUMARTS to integer.\n",
    "                        file_path = self.bucket_path + key.key\n",
    "                        spark_df1 = spark.read.csv(file_path, header=True, sep=\"\\t\")\n",
    "                        print(\"Showing results of Spark DataFrame 1: \\n\")                        \n",
    "#                         # FILTER OUT BAD RECORDS\n",
    "#                         spark_df1_columns = [\"DATE\", \"NUMARTS\", \"COUNTS\", \"THEMES\", \"LOCATIONS\", \"PERSONS\", \n",
    "#                                              \"ORGANIZATIONS\", \"TONE\", \"CAMEOEVENTIDS\", \"SOURCES\", \"SOURCEURLS\"]\n",
    "                        \n",
    "#                         # IF VALUES EXIST INSIDE SPARK DATAFRAME, OR IF SPARK DATAFRAME COLUMNS EXIST INSIDE COLUMNS LIST?\n",
    "#                         counter = 0\n",
    "#                         for col in spark_df1.columns:\n",
    "#                             counter += 1\n",
    "#                             if col not in spark_df1_columns:\n",
    "#                                 # Save file to desired location\n",
    "#                                 save_location = \"s3://oecbucket2/Bad_Files/\"\n",
    "#                                 file_location = save_location + key.key\n",
    "#                                 spark_df1.repartition(1).write.csv(path=file_location, header=True, sep=\"\\t\")\n",
    "#                                 print(\"Wrote to CSV File.\")\n",
    "#                                 counter = 0\n",
    "#                                 break\n",
    "                                \n",
    "#                         # Apply Transformations    \n",
    "#                         if counter == (len(spark_df1_columns) - 1):\n",
    "#                             spark_df1 = spark_df1.withColumn(\"NUMARTS\", spark_df1[\"NUMARTS\"].cast(\"int\"))\\\n",
    "#                             .drop(\"DATE\", \"COUNTS\")\n",
    "#                             print(\"Spark DF 1 Transformed.\")\n",
    "#                             # Write Back to CSV file\n",
    "#                             save_location = \"s3://oecbucket2/Data_Stream_1/\"\n",
    "#                             file_location = save_location + key.key\n",
    "#                             spark_df1.repartition(1).write.csv(path=file_location, header=True, sep=\"\\t\")\n",
    "#                             print(\"Wrote to CSV File.\")    \n",
    " \n",
    "                        # Apply Transformations\n",
    "                        spark_df1 = spark_df1.withColumn(\"NUMARTS\", spark_df1[\"NUMARTS\"].cast(\"int\"))\\\n",
    "                        .drop(\"DATE\", \"COUNTS\")\n",
    "                        print(\"Spark DF 1 Transformed.\")\n",
    "#                         spark_df1.limit(3).show()\n",
    "                        # Write Back to CSV file\n",
    "                        save_location = \"s3://oecbucket2/Data_Stream_1/\"\n",
    "                        file_location = save_location + key.key\n",
    "                        spark_df1.repartition(1).write.csv(path=file_location, header=True, sep=\"\\t\")\n",
    "                        print(\"Wrote to CSV File.\")    \n",
    "        \n",
    "    def s3_copy_delete(self):\n",
    "        \"\"\"\n",
    "        This method will copy and delete files from S3 bucket to ensure I have\n",
    "        desired files inside Data_Stream_1 Folder and Data_Stream_2 Folder.\n",
    "        \"\"\"\n",
    "        # Alternative approach\n",
    "        s3 = self.s3_resource\n",
    "        # s3 = self.s3_cli_object\n",
    "        s3_bucket_name = self.s3_bucket_name\n",
    "\n",
    "        for bucket in s3.buckets.all():\n",
    "            # print(bucket)\n",
    "            if bucket == s3.Bucket(name='oecbucket2'):\n",
    "                print(\"OEC.\")\n",
    "                # Try to do more stuff.\n",
    "                for key in bucket.objects.all():\n",
    "                    # print(key.key)\n",
    "                    if \"Data_Stream_2/\" and \"part\" in key.key:\n",
    "                        print(key.key)\n",
    "                        # Split string on \"/\" and take first two                         \n",
    "                        file_to_move = key.key\n",
    "                        file_char = file_to_move.split(\"/\")\n",
    "                        # print(file_char[0:2])\n",
    "                        sep_list = file_char[:]\n",
    "                        insert_at = 1\n",
    "                        sep_list[insert_at:insert_at] = [\"/\"]\n",
    "                        print(sep_list)\n",
    "                        file_char = sep_list[0:3]\n",
    "                        file_str = \"\".join(file_char)\n",
    "                        print(file_str)\n",
    "                        s3.Object(\"oecbucket2\", file_str)\\\n",
    "                         .copy_from(CopySource=\"oecbucket2/\" + key.key)\n",
    "                        s3.Object(\"oecbucket2\", key.key).delete()\n",
    "                    elif \"Data_Stream_1/\" and \"part\" in key.key:\n",
    "                        print(key.key)\n",
    "                        # Split string on \"/\" and take first two                         \n",
    "                        file_to_move = key.key\n",
    "                        file_char = file_to_move.split(\"/\")\n",
    "                        # print(file_char[0:2])\n",
    "                        sep_list = file_char[:]\n",
    "                        insert_at = 1\n",
    "                        sep_list[insert_at:insert_at] = [\"/\"]\n",
    "                        print(sep_list)\n",
    "                        file_char = sep_list[0:3]\n",
    "                        file_str = \"\".join(file_char)\n",
    "                        print(file_str)\n",
    "                        s3.Object(\"oecbucket2\", file_str)\\\n",
    "                         .copy_from(CopySource=\"oecbucket2/\" + key.key)\n",
    "                        s3.Object(\"oecbucket2\", key.key).delete()\n",
    "    \n",
    "    \n",
    "    def remove_zip_files(self):\n",
    "        \"\"\"\n",
    "        Sample of what to do if I have zip files.\n",
    "        This needs to be updated, but it is good to think about what I can do.\n",
    "        \"\"\"\n",
    "        # Cloud path to unzip\n",
    "        cloud_path = self.s3_bucket_path\n",
    "        # Get a list of all the file paths that ends with .txt from in specified directory\n",
    "        file_list = glob.glob(cloud_path + '*.zip')\n",
    "        # Iterate over the list of file paths & remove each file.\n",
    "        for file_path in file_list:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error removing file: {e}\")\n",
    "                \n",
    "    def remove_file_folders(self):\n",
    "        \"\"\"\n",
    "        This method will be used to remove all file folders.\n",
    "        \"\"\"\n",
    "        s3 = self.s3_resource\n",
    "        s3_bucket_name = self.s3_bucket_name\n",
    "\n",
    "        for bucket in s3.buckets.all():\n",
    "            # print(bucket)\n",
    "            if bucket == s3.Bucket(name='oecbucket2'):\n",
    "                print(\"OEC.\")\n",
    "                for key in bucket.objects.all():\n",
    "#                     print(key.key)\n",
    "                    if \"Data_Stream_1\" and \".csv/\" in key.key:\n",
    "                        print(key.key[:-8])\n",
    "                        bucket.objects.filter(Prefix=key.key[:-8]).delete()\n",
    "                        print(\"Data_Stream_1 Subfolders deleted.\")\n",
    "                    elif \"Data_Stream_2\" and \".csv/\" in key.key:\n",
    "                        print(key.key[:-8])\n",
    "                        bucket.objects.filter(Prefix=key.key[:-8]).delete()\n",
    "                        print(\"Data_Stream_2 Subfolders deleted.\")\n",
    "                        \n",
    "    def remove_other_files(self):\n",
    "        s3 = self.s3_resource\n",
    "        s3_bucket_name = self.s3_bucket_name\n",
    "\n",
    "        for bucket in s3.buckets.all():\n",
    "            # print(bucket)\n",
    "            if bucket == s3.Bucket(name='oecbucket2'):\n",
    "                print(\"OEC.\")\n",
    "                for key in bucket.objects.all():\n",
    "                    if (\"Data_Stream_1\" not in key.key) and (\".csv\" in key.key):\n",
    "                        print(key.key[:-8])\n",
    "                        bucket.objects.filter(Prefix=key.key[:-8]).delete()\n",
    "                        print(\"Other Files deleted.\")\n",
    "                    elif (\"Data_Stream_2\" not in key.key) and (\".csv\" in key.key):\n",
    "                        print(key.key[:-8])\n",
    "                        bucket.objects.filter(Prefix=key.key[:-8]).delete()\n",
    "                        print(\"Other files deleted.\")\n",
    "                        \n",
    "    def remove_all_files(self):\n",
    "        s3 = self.s3_resource\n",
    "        s3_bucket_name = self.s3_bucket_name\n",
    "\n",
    "        for bucket in s3.buckets.all():\n",
    "            # print(bucket)\n",
    "            if bucket == s3.Bucket(name='oecbucket2'):\n",
    "                print(\"OEC.\")\n",
    "                for key in bucket.objects.all():\n",
    "                    if \".csv\" in key.key:\n",
    "                        print(key.key[:-8])\n",
    "                        bucket.objects.filter(Prefix=key.key[:-8]).delete()\n",
    "                        print(\"All Files deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1495f33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a798f044364d839d3011d77b92d465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instead, create S3 client object\n",
    "s3_client_obj = boto3.client(\"s3\")\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "# S3 Bucket name\n",
    "s3_bucket_name = \"oecbucket2\"\n",
    "\n",
    "# Instead, use AWS Educate's S3 Bucket path\n",
    "# s3_bucket_path = \"s3a://openendedcapstone/\"\n",
    "s3_bucket_path = \"s3://oecbucket2/\"\n",
    "\n",
    "# Obtain url\n",
    "website_url = \"http://data.gdeltproject.org/gkg/index.html\"\n",
    "\n",
    "# Create data pipeline (use s3 object as input) - this will ask if you want to using s3 boto3 object and s3_uri_path object.\n",
    "dp = DataPipeline(s3_cli_object=s3_client_obj, s3_resource=s3_resource, s3_bucket=s3_bucket_name, bucket_path=s3_bucket_path, url=website_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6d9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b860742ea85b401d80720897c4adbd61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665209b494f845838b2f8ef6c88ac912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp.extract_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.spark_read_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.s3_copy_delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bed5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.remove_other_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
